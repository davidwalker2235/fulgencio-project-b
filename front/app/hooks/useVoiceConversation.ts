import { useState, useRef, useCallback, useEffect } from "react";
import { useWebSocket } from "./useWebSocket";
import { useAudioRecording } from "./useAudioRecording";
import { useAudioPlayback } from "./useAudioPlayback";
import { WEBSOCKET_URL, VOICE_DETECTION } from "../constants";
import { Message, ConnectionStatus, WebSocketMessage } from "../types";
import {
  arrayBufferToFloat32,
  base64ToFloat32,
} from "../services/audioUtils";
import { useFirebase } from "./useFirebase";

interface UseVoiceConversationReturn {
  isConnected: boolean;
  isRecording: boolean;
  transcription: Message[];
  error: string;
  connectionStatus: ConnectionStatus;
  isSpeaking: boolean;
  resolvedCaricatures: string[];
  resolvedPhoto: string | null;
  activeUserId: string | null;
  startConversation: () => Promise<void>;
  stopConversation: (transcripci√≥n: Message[]) => void;
  toggleConversation: (transcripci√≥n: Message[]) => void;
  clearError: () => void;
  sendTextMessage: (text: string) => void;
}

/**
 * Hook principal que orquesta toda la l√≥gica de conversaci√≥n de voz
 */
export function useVoiceConversation(): UseVoiceConversationReturn {
  const [isConnected, setIsConnected] = useState(false);
  const [isRecording, setIsRecording] = useState(false);
  const [transcription, setTranscription] = useState<Message[]>([]);
  const [error, setError] = useState<string>("");
  const [connectionStatus, setConnectionStatus] =
    useState<ConnectionStatus>("Disconnected");
  const [isSpeaking, setIsSpeaking] = useState(false);
  const [resolvedCaricatures, setResolvedCaricatures] = useState<string[]>([]);
  const [resolvedPhoto, setResolvedPhoto] = useState<string | null>(null);

  const {
    connect,
    disconnect,
    send,
    onMessage,
    onConnection,
    isConnected: wsIsConnected,
  } = useWebSocket();
  const { startRecording, stopRecording, isRecording: audioIsRecording } =
    useAudioRecording();
  const { playAudio, stopAllAudio, hasActiveAudio } = useAudioPlayback();
  const { write, read, subscribe, loading, error: firebaseError } = useFirebase();

  const currentResponseIdRef = useRef<string | null>(null);
  const isUserSpeakingRef = useRef<boolean>(false);
  const isInterruptedRef = useRef<boolean>(false);
  const silenceTimerRef = useRef<NodeJS.Timeout | null>(null);
  const audioCheckIntervalRef = useRef<NodeJS.Timeout | null>(null);
  const [activeUserId, setActiveUserId] = useState<string | null>(null);

  // Monitorear el estado del audio para actualizar isSpeaking
  useEffect(() => {
    audioCheckIntervalRef.current = setInterval(() => {
      const hasAudio = hasActiveAudio();
      setIsSpeaking(hasAudio);
    }, 100); // Verificar cada 100ms

    return () => {
      if (audioCheckIntervalRef.current) {
        clearInterval(audioCheckIntervalRef.current);
      }
    };
  }, [hasActiveAudio]);

  // Limpiar recursos al desmontar
  useEffect(() => {
    return () => {
      if (silenceTimerRef.current) {
        clearTimeout(silenceTimerRef.current);
      }
      if (audioCheckIntervalRef.current) {
        clearInterval(audioCheckIntervalRef.current);
      }
      // Detener grabaci√≥n y desconectar
      stopRecording();
      disconnect();
      stopAllAudio();
    };
    // eslint-disable-next-line react-hooks/exhaustive-deps
  }, []);

  const handleAudioChunk = useCallback(
    (audioData: ArrayBuffer) => {
      if (wsIsConnected() && audioIsRecording()) {
        send(audioData);
      }
    },
    [send, wsIsConnected, audioIsRecording]
  );

  const handleUserSpeaking = useCallback(
    (isSpeaking: boolean, wasSpeaking: boolean) => {
      const audioIsActive = hasActiveAudio();

      // Si el usuario empieza a hablar mientras la IA est√° hablando, cancelar INMEDIATAMENTE
      if (isSpeaking && !wasSpeaking && audioIsActive) {
        console.log("üö® INTERRUPCI√ìN DETECTADA - Usuario hablando mientras IA habla");
        isInterruptedRef.current = true;
        stopAllAudio();

        // Si hay una respuesta activa, cancelarla en el servidor
        if (currentResponseIdRef.current) {
          try {
            send({
              type: "response.cancel",
              response_id: currentResponseIdRef.current,
            });
            console.log("‚úÖ Comando de cancelaci√≥n enviado al servidor");
          } catch (err) {
            console.error("‚ùå Error enviando cancelaci√≥n:", err);
          }
        }

        // Limpiar timer de silencio si existe
        if (silenceTimerRef.current) {
          clearTimeout(silenceTimerRef.current);
          silenceTimerRef.current = null;
        }
      }

      // Si el usuario deja de hablar, quitar la marca de interrupci√≥n
      if (!isSpeaking && wasSpeaking) {
        isInterruptedRef.current = false;
      }

      // Si el usuario deja de hablar, esperar y solicitar respuesta
      if (!isSpeaking && wasSpeaking) {
        // Limpiar timer anterior si existe
        if (silenceTimerRef.current) {
          clearTimeout(silenceTimerRef.current);
        }
        // Esperar silencio antes de solicitar respuesta
        silenceTimerRef.current = setTimeout(() => {
          if (wsIsConnected() && !currentResponseIdRef.current) {
            console.log("Usuario dej√≥ de hablar - solicitando respuesta");
            send({
              type: "response.create",
            });
          }
          silenceTimerRef.current = null;
        }, VOICE_DETECTION.silenceDurationMs);
      }
    },
    [send, wsIsConnected, stopAllAudio, hasActiveAudio]
  );

  // Funci√≥n para generar un ID de usuario √∫nico
  const generateUserId = useCallback((): string => {
    // Generar ID √∫nico usando timestamp y random
    const timestamp = Date.now();
    const random = Math.random().toString(36).substring(2, 15);
    return `user_${timestamp}_${random}`;
  }, []);

  const startConversation = useCallback(async () => {
    try {
      setError("");
      setConnectionStatus("Connecting");

      // Generar ID √∫nico para este usuario/sesi√≥n
      const userId = generateUserId();
      setActiveUserId(userId);
      setResolvedCaricatures([]);
      console.log("üÜî ID de usuario generado:", userId);

      // Configurar handlers de mensajes WebSocket ANTES de conectar
      // Los handlers se guardar√°n y se aplicar√°n cuando se cree el servicio
      console.log("üìù Registrando handlers de mensajes...");
      
      // Handler gen√©rico para debug - capturar todos los mensajes
      onMessage("*", (data: WebSocketMessage) => {
        console.log("üîç Mensaje gen√©rico recibido:", data.type, data);
      });
      
      onMessage("audio", async (blob: Blob) => {
        console.log("üîä Handler de audio ejecutado");
        if (isInterruptedRef.current) {
          console.log("‚è∏Ô∏è Audio interrumpido, ignorando");
          return;
        }

        try {
          const arrayBuffer = await blob.arrayBuffer();
          const float32 = await arrayBufferToFloat32(arrayBuffer);
          console.log("‚ñ∂Ô∏è Reproduciendo audio, tama√±o:", float32.length);
          playAudio(float32);
        } catch (audioErr) {
          console.error("Error reproduciendo audio:", audioErr);
        }
      });

      onMessage("conversation.item.input_audio_transcription.completed", (data: WebSocketMessage) => {
        const userMessage: Message = {
          role: "user",
          content: (data.transcript as string) || "",
          timestamp: new Date(),
        };
        setTranscription((prev) => [...prev, userMessage]);
      });

      onMessage("user.context.resolved", (data: WebSocketMessage) => {
        if (Array.isArray(data.caricatures)) {
          const cleaned = data.caricatures.filter(
            (img: unknown) => typeof img === "string" && img.trim().length > 0
          ) as string[];
          console.log("üñºÔ∏è Caricaturas recibidas en frontend:", cleaned.length);
          setResolvedCaricatures(cleaned);
        } else {
          setResolvedCaricatures([]);
        }
        
        if (typeof data.photo === "string" && data.photo.trim().length > 0) {
          console.log("üì∏ Foto del usuario recibida en frontend");
          setResolvedPhoto(data.photo);
        } else {
          setResolvedPhoto(null);
        }
      });

      // Handler para cuando se completa el procesamiento de un mensaje de texto
      onMessage("conversation.item.input_text.done", (data: WebSocketMessage) => {
        console.log("‚úÖ Mensaje de texto procesado:", data);
        // El mensaje ya deber√≠a estar en la transcripci√≥n, solo confirmamos
      });

      onMessage("response.audio.delta", (data: WebSocketMessage) => {
        console.log("üîä Handler de audio delta ejecutado");
        if (isInterruptedRef.current) {
          console.log("‚è∏Ô∏è Audio interrumpido, ignorando delta");
          return;
        }

        try {
          const float32 = base64ToFloat32((data.delta as string) || "");
          console.log("‚ñ∂Ô∏è Reproduciendo audio delta, tama√±o:", float32.length);
          playAudio(float32);
        } catch (audioErr) {
          console.error("Error procesando audio delta:", audioErr);
        }
      });

      onMessage("conversation.item.output_text.delta", (data: WebSocketMessage) => {
        console.log("üìù Delta de texto recibido:", data);
        const deltaText = (data.delta as string) || "";
        console.log("üìù Contenido delta:", deltaText);
        
        if (!deltaText || deltaText.trim() === "") {
          console.log("‚ö†Ô∏è Delta vac√≠o, ignorando");
          return;
        }
        
        setTranscription((prev) => {
          console.log("üìù Estado anterior:", prev);
          const lastMessage = prev[prev.length - 1];
          console.log("üìù √öltimo mensaje:", lastMessage);
          
          if (lastMessage && lastMessage.role === "assistant") {
            const updated = [
              ...prev.slice(0, -1),
              {
                ...lastMessage,
                content: lastMessage.content + deltaText,
              },
            ];
            console.log("üìù Actualizando mensaje existente de asistente, nuevo contenido:", updated[updated.length - 1].content);
            return updated;
          } else {
            const newMessage = {
              role: "assistant" as const,
              content: deltaText,
              timestamp: new Date(),
            };
            console.log("üìù Creando nuevo mensaje de asistente:", newMessage);
            return [...prev, newMessage];
          }
        });
      });

      onMessage("conversation.item.output_text.done", (data: WebSocketMessage) => {
        console.log("‚úÖ Texto completo recibido:", data);
        const fullText = (data.text as string) || "";
        console.log("‚úÖ Contenido completo:", fullText);
        
        setTranscription((prev) => {
          const lastMessage = prev[prev.length - 1];
          console.log("‚úÖ √öltimo mensaje antes de done:", lastMessage);
          
          if (lastMessage && lastMessage.role === "assistant") {
            const updated = [
              ...prev.slice(0, -1),
              {
                ...lastMessage,
                content: fullText || lastMessage.content,
              },
            ];
            console.log("‚úÖ Actualizando mensaje final de asistente");
            return updated;
          } else {
            const newMessage = {
              role: "assistant" as const,
              content: fullText,
              timestamp: new Date(),
            };
            console.log("‚úÖ Creando nuevo mensaje final de asistente:", newMessage);
            return [...prev, newMessage];
          }
        });
      });

      // Handler para transcripci√≥n de audio de la IA (si viene como audio_transcript)
      onMessage("response.audio_transcript.delta", (data: WebSocketMessage) => {
        console.log("üé§ Transcripci√≥n de audio delta recibida:", data);
        const transcriptDelta = (data.delta as string) || "";
        console.log("üé§ Contenido delta de transcripci√≥n:", transcriptDelta);
        
        if (!transcriptDelta || transcriptDelta.trim() === "") {
          return;
        }
        
        setTranscription((prev) => {
          const lastMessage = prev[prev.length - 1];
          
          if (lastMessage && lastMessage.role === "assistant") {
            return [
              ...prev.slice(0, -1),
              {
                ...lastMessage,
                content: lastMessage.content + transcriptDelta,
              },
            ];
          } else {
            return [
              ...prev,
              {
                role: "assistant",
                content: transcriptDelta,
                timestamp: new Date(),
              },
            ];
          }
        });
      });

      onMessage("response.audio_transcript.done", (data: WebSocketMessage) => {
        console.log("üé§ Transcripci√≥n de audio completa recibida:", data);
        const fullTranscript = (data.transcript as string) || "";
        console.log("üé§ Contenido completo de transcripci√≥n:", fullTranscript);
        
        if (!fullTranscript || fullTranscript.trim() === "") {
          return;
        }
        
        setTranscription((prev) => {
          const lastMessage = prev[prev.length - 1];
          
          if (lastMessage && lastMessage.role === "assistant") {
            return [
              ...prev.slice(0, -1),
              {
                ...lastMessage,
                content: fullTranscript || lastMessage.content,
              },
            ];
          } else {
            return [
              ...prev,
              {
                role: "assistant",
                content: fullTranscript,
                timestamp: new Date(),
              },
            ];
          }
        });
      });

      onMessage("response.created", (data: WebSocketMessage) => {
        currentResponseIdRef.current =
          (data.response as { id?: string })?.id || null;
        console.log("Respuesta creada:", currentResponseIdRef.current);
      });

      onMessage("response.done", () => {
        console.log("Respuesta completada");
        currentResponseIdRef.current = null;
      });

      onMessage("response.cancelled", () => {
        console.log("Respuesta cancelada por el servidor");
        currentResponseIdRef.current = null;
        stopAllAudio();
        isInterruptedRef.current = false;
      });

      onMessage("error", (data: WebSocketMessage) => {
        setError((data.message as string || data.error.message as string) || "Error desconocido");
        setConnectionStatus("Disconnected");
      });

      // Configurar handlers de conexi√≥n ANTES de conectar
      // El hook guardar√° los handlers y los aplicar√° cuando se cree el servicio
      onConnection({
        onOpen: () => {
          console.log("‚úÖ WebSocket conectado - actualizando estado");
          setIsConnected(true);
          setConnectionStatus("Connected");
          setIsRecording(true);

          // Resetear estados
          currentResponseIdRef.current = null;
          isUserSpeakingRef.current = false;
          isInterruptedRef.current = false;
          if (silenceTimerRef.current) {
            clearTimeout(silenceTimerRef.current);
            silenceTimerRef.current = null;
          }
        },
        onClose: () => {
          console.log("‚ùå WebSocket cerrado");
          setIsConnected(false);
          setConnectionStatus("Disconnected");
          setIsRecording(false);
        },
        onError: (err: Error) => {
          console.error("‚ùå Error en WebSocket:", err);
          setError(err.message);
          setConnectionStatus("Disconnected");
        },
      });

      // Ahora conectar (el servicio se crear√° y aplicar√° los handlers guardados)
      await connect(WEBSOCKET_URL);

      // Iniciar grabaci√≥n de audio
      await startRecording(handleAudioChunk, handleUserSpeaking);
    } catch (err) {
      console.error("Error iniciando conversaci√≥n:", err);
      setError(
        err instanceof Error
          ? err.message
          : "Error al acceder al micr√≥fono o conectar con el servidor"
      );
      setConnectionStatus("Disconnected");
    }
  }, [
    connect,
    disconnect,
    send,
    onMessage,
    onConnection,
    startRecording,
    handleAudioChunk,
    playAudio,
    stopAllAudio,
    wsIsConnected,
    audioIsRecording,
    hasActiveAudio,
    generateUserId,
  ]);

  const stopConversation = useCallback((transcription: Message[]) => {
    console.log("Deteniendo conversaci√≥n...");

    // Detener todo el audio inmediatamente
    stopAllAudio();

    // Cancelar respuesta activa si existe
    if (currentResponseIdRef.current && wsIsConnected()) {
      console.log("Cancelando respuesta activa antes de cerrar");
      send({
        type: "response.cancel",
        response_id: currentResponseIdRef.current,
      });
    }

    // Cerrar WebSocket
    disconnect();

    // Detener grabaci√≥n
    stopRecording();

    // Limpiar timers
    if (silenceTimerRef.current) {
      clearTimeout(silenceTimerRef.current);
      silenceTimerRef.current = null;
    }

    // Guardar la transcripci√≥n en la base de datos usando el ID del usuario activo
    if (activeUserId) {
      const timestamp = Date.now();
      write(`users/${activeUserId}/transcriptions/${timestamp}`, transcription);
      console.log(`Transcripci√≥n guardada en users/${activeUserId}/transcriptions/${timestamp}`);
      
      // Limpiar el ID del usuario activo despu√©s de guardar
      setActiveUserId(null);
    } else {
      console.warn("‚ö†Ô∏è No hay ID de usuario activo, no se guardar√° la transcripci√≥n");
    }

    // Resetear estados
    setIsRecording(false);
    setIsConnected(false);
    setConnectionStatus("Disconnected");
    currentResponseIdRef.current = null;
    isUserSpeakingRef.current = false;
    setResolvedCaricatures([]);
    setTranscription([]);
  }, [disconnect, stopRecording, stopAllAudio, send, wsIsConnected, write, activeUserId]);

  const toggleConversation = useCallback((transcription: Message[]) => {
    if (isRecording) {
      stopConversation(transcription);
    } else {
      startConversation();
    }
  }, [isRecording, startConversation, stopConversation]);

  const clearError = useCallback(() => {
    setError("");
  }, []);

  const sendTextMessage = useCallback(
    (text: string) => {
      if (!wsIsConnected() || !text.trim()) {
        console.warn("No se puede enviar texto: WebSocket no conectado o texto vac√≠o");
        if (!wsIsConnected()) {
          setError("No hay conexi√≥n activa. Por favor, inicia una conversaci√≥n primero.");
        }
        return;
      }

      // Agregar mensaje del usuario a la transcripci√≥n inmediatamente
      const userMessage: Message = {
        role: "user",
        content: text.trim(),
        timestamp: new Date(),
      };
      setTranscription((prev) => [...prev, userMessage]);

      // Enviar texto a GPT Realtime usando conversation.item.create
      const textMessage: WebSocketMessage = {
        type: "conversation.item.create",
        item: {
          type: "message",
          role: "user",
          content: [
            {
              type: "input_text",
              text: text.trim(),
            },
          ],
        },
      };

      send(textMessage);
      console.log("üì§ Texto enviado a GPT Realtime:", text.trim());

      // Solicitar respuesta despu√©s de enviar el texto
      // Esperar un momento para que el mensaje se procese
      setTimeout(() => {
        if (wsIsConnected() && !currentResponseIdRef.current) {
          send({
            type: "response.create",
          });
          console.log("‚úÖ Solicitud de respuesta enviada despu√©s de texto");
        }
      }, 100);
    },
    [send, wsIsConnected]
  );

  return {
    isConnected,
    isRecording,
    transcription,
    error,
    connectionStatus,
    isSpeaking,
    resolvedCaricatures,
    resolvedPhoto,
    activeUserId,
    startConversation,
    stopConversation,
    toggleConversation,
    clearError,
    sendTextMessage,
  };
}

